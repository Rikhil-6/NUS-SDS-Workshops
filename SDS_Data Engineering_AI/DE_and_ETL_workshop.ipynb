{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTRvObE8KGQy"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "import io\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Data Engineering**\n",
        "\n",
        "Data Engineering involves the collection, transformation, and management of data to facilitate analytics and machine learning applications. \n",
        "In this notebook, data engineering principles are applied to extract and process genomic data stored in AWS S3.\n",
        "\n",
        "Key components covered in this notebook include:\n",
        "\n",
        "- Data Extraction from AWS S3: The dataset is retrieved using Boto3, allowing anonymous access to S3 buckets.\n",
        "\n",
        "- Data Processing: The extracted GTF data follows a structured format with standardized columns such as seqname, source, feature, start, end, score, strand, and frame. This structure ensures consistency and usability in downstream analysis.\n",
        "\n",
        "- Transformation & Visualization: The dataset is processed using Pandas, and key insights are visualized using Matplotlib. This step is crucial for understanding data distributions and patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqtn0Rh6ig-8"
      },
      "outputs": [],
      "source": [
        "# Connect to S3 anonymously\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ind45H5rXQ9s"
      },
      "outputs": [],
      "source": [
        "def load_gtf_data(bucket_name, file_key): # Load in the information using bucket and key information on AWS\n",
        "    try:\n",
        "        response = s3.get_object(Bucket= ---- , Key= ----) # Notice the '----' signs? That's code for you to fill in!\n",
        "        # fill in the get_object method with the appropriate variables\n",
        "        gtf_data = response['Body'].read().decode('utf-8')\n",
        "        return gtf_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data from S3: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcWZcWRyq8pU"
      },
      "source": [
        "The GTF data has the following standardised column names together with their attributes\n",
        "\n",
        "1. `seqname`: Name of the sequence (e.g., chromosome).\n",
        "2. `source`: Source of the annotation (e.g., a specific database or tool).\n",
        "3. `feature`: Type of feature (e.g., gene, exon).\n",
        "4. `start`: Start position of the feature.\n",
        "5. `end`: End position of the feature.\n",
        "6. `score`: Score associated with the feature (often a placeholder).\n",
        "7. `strand`: Strand of the feature (+ or -).\n",
        "8. `frame`: Reading frame (0, 1, 2).\n",
        "9. `attribute`: Additional attributes in a key-value format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVnQoJqikp-e"
      },
      "outputs": [],
      "source": [
        "def parse_gtf_data(gtf_data): # Retrieve the relevant data and Load it Locally into a csv file\n",
        "    col_names = [\n",
        "        ----, ----, ----, \"start\", \"end\", # some of the column names here are empty --> fill them in as need be!!\n",
        "        \"score\", \"strand\", \"frame\", \"attribute\"\n",
        "    ]\n",
        "    gtf_df = pd.read_csv(io.StringIO(gtf_data), sep=\"\\t\", comment=\"#\", names=col_names)\n",
        "    # The io library in Python allows for the quick (and even memoryless) retrieval of relevant data through boto's response\n",
        "    return gtf_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArBaKDo3kqyZ"
      },
      "outputs": [],
      "source": [
        "def filter_genes(gtf_df): # Attains the features of interest and get the relevant ID base information\n",
        "  genes_df = gtf_df[gtf_df[----] == ----].copy() # What feature shld be filtered to get the relevant information as need be?\n",
        "  genes_df[\"gene_id\"] = genes_df[\"attribute\"].str.extract('gene_id \"([^\"]+)\"') # Regex is employed here - but can u think of other ways\n",
        "  # that the code can be amended while being functional?\n",
        "  return genes_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBt5qm_io-rw"
      },
      "outputs": [],
      "source": [
        "def extract_attributes(df): # Amend the code as would appear logically!\n",
        "    df[----] = df['attribute'].astype(str)\n",
        "    df[----] = df['attribute'].str.extract('gene_name \"([^\"]+)\"')\n",
        "    df[----] = df['attribute'].str.extract('gene_biotype \"([^\"]+)\"')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvVY2hWRjw_H"
      },
      "outputs": [],
      "source": [
        "def clean_data(df):\n",
        "    df.----(inplace=True) # A method is being used here to remove values that occur more than once -> implement that via code!\n",
        "    df.----(method='ffill', inplace=True) # A method is being used here to fill up missing values -> implement that via code!\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bx29ccLpEF1"
      },
      "outputs": [],
      "source": [
        "def normalize_data(df):\n",
        "    df['start_norm'] = (df['start'] - df['start'].min()) / (df['start'].max() - df['start'].min()) # the start col has been normalised\n",
        "    df['end_norm'] = ---- # Try it out for the end column as well!\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-kp7VTtnb1T"
      },
      "outputs": [],
      "source": [
        "def validate_data(df):\n",
        "    print(\"Missing values:\\n\", df.isnull().sum())\n",
        "    print(\"\\nData types:\\n\", df.dtypes)\n",
        "    print(\"\\nUnique values in 'feature':\\n\", df['feature'].unique())\n",
        "    print(\"\\nRange of 'start' and 'end':\\n\", df[['start', 'end']].agg(['min', 'max']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vbaKRT8j3im"
      },
      "outputs": [],
      "source": [
        "def aggregate_data(df): # Group data together by specific biotypes\n",
        "    agg_df = df.groupby('gene_biotype').agg({\n",
        "        'start': [----, ----], # Get useful metrics for attaining statistical information for the start and end columns \n",
        "        'end': [----, ----] # Try out 2 metrics each - though feel free to include more!!\n",
        "    }).reset_index()\n",
        "    print(\"Aggregated data:\\n\", agg_df)\n",
        "    return agg_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3VHGJ-mnfC3"
      },
      "outputs": [],
      "source": [
        "def explore_data(df): # Visualise useful information from the dataframe\n",
        "    print(\"Summary statistics:\\n\", df.describe())\n",
        "    df['feature'].value_counts().plot(kind='bar')\n",
        "    plt.title('Distribution of Features')\n",
        "    plt.xlabel('Feature')\n",
        "    plt.ylabel('Count')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j82V3loxnoBx"
      },
      "outputs": [],
      "source": [
        "def filter_long_genes(genes_df, min_length=1000):\n",
        "    genes_df['length'] = genes_df['end'] - genes_df['start']\n",
        "    long_genes_df = genes_df[----] # given the min_length parameter - > how would you filter the dataframe to only get long genes?\n",
        "    return long_genes_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIAWOVPtntUK"
      },
      "outputs": [],
      "source": [
        "def save_data(df, filename):\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Data saved as '{filename}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "s07-3k60nxgk",
        "outputId": "714991e4-8131-4801-ed4b-ca8a33f50c15"
      },
      "outputs": [],
      "source": [
        "# Main pipeline\n",
        "bucket_name = \"sg-nex-data\"\n",
        "file_key = \"data/data_tutorial/annotations/hg38_chr22.gtf\"\n",
        "\n",
        "# Fill in the method names below accordingly to create a useful pipeline structure!!\n",
        "gtf_data = -----(bucket_name, file_key)\n",
        "if gtf_data:\n",
        "    gtf_df = -----(gtf_data)\n",
        "    gtf_df = clean_data(gtf_df)\n",
        "    validate_data(gtf_df)\n",
        "    explore_data(gtf_df)\n",
        "    genes_df = filter_genes(gtf_df)\n",
        "    genes_df = -------(genes_df)\n",
        "    genes_df = normalize_data(genes_df)\n",
        "    agg_df = aggregate_data(genes_df)\n",
        "    long_genes_df = ------(genes_df)\n",
        "    save_data(----, \"data/filtered_chr22_genes.csv\")\n",
        "    save_data(-----, \"data/long_chr22_genes.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Conclusion**\n",
        "\n",
        "We hope to present this as a testament to the power of modern data engineering. By seamlessly integrating cloud-based storage, efficient retrieval strategies, and structured data transformations, we unlock the potential hidden within various datasets. \n",
        "\n",
        "The techniques showcased here lay the groundwork for further analytics and machine learning to be performed, pushing the boundaries of whatâ€™s possible in genomic research and beyond. With the right tools and methodologies, data engineers wield the power to turn raw information into groundbreaking discoveries."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
