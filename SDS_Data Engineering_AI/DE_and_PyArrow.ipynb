{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import time\n",
    "import boto3\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Pipeline Process**\n",
    "\n",
    "A data pipeline typically follows an ETL (Extract, Transform, Load) like workflow. In this case, data is first extracted from AWS S3 using Boto3. It is then parsed into a structured Pandas DataFrame for processing. \n",
    "\n",
    "The pipeline filters relevant data, extracting attributes such as `gene_id` and `gene_name`. Finally, the cleaned dataset is converted into Apache Arrow format and stored as a Parquet file for efficient storage and retrieval. Using Arrow and Parquet significantly improves read/write performance compared to traditional formats like CSV. This structured approach ensures scalability, data integrity, and quick access for analytics and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning) # prevent warnings from popping up and obfuscating results\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))  \n",
    "# Connect anonymously to s3 --> since data is public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gtf_data(bucket_name, file_key): # reads content from AWS\n",
    "    try:\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=file_key) # fetches the files from s3\n",
    "        gtf_data = response['Body'].read().decode('utf-8')\n",
    "        return gtf_data\n",
    "    except Exception as e: # error-handling\n",
    "        print(f\"Error fetching data from S3: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gtf_data(gtf_text): #converts the text into a pandas dataframe\n",
    "    col_names = [\"seqname\", \"source\", \"feature\", \"start\", \"end\", \"score\", \"strand\", \"frame\", \"attribute\"]\n",
    "    df = pd.read_csv(io.StringIO(gtf_text), sep='\\t', comment='#', names=col_names, low_memory=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_genes(df):\n",
    "    return df[df['feature'] == 'gene'] \n",
    "    # we want rows where the feature is \"gene\" only to reduce dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(df): \n",
    "    #to return a cleaned dataframe with only essential columns (varies according to function)\n",
    "    df.loc[:,'gene_id'] = df['attribute'].str.extract('gene_id \"(.*?)\"')\n",
    "    df.loc[:,'gene_name'] = df['attribute'].str.extract('gene_name \"(.*?)\"')\n",
    "    df.loc[:,'gene_biotype'] = df['attribute'].str.extract('gene_biotype \"(.*?)\"')\n",
    "    return df[['seqname', 'start', 'end', 'strand', 'gene_id', 'gene_name', 'gene_biotype']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyArrow and Data Parsing\n",
    "\n",
    "PyArrow is a very powerful library commonly used in a variety of data-science and DE projects which provides efficient in-memory data structures based on the Apache Arrow format. It enables zero-copy reads, which means data can be processed without unnecessary duplication, making it significantly faster than traditional row-based formats.\n",
    "\n",
    "PyArrow works well on Parquet based files. Parquet, a columnar storage format, is optimized for analytics and reduces I/O overhead by enabling selective column reads. Compared to Pandas, which relies on row-based processing, the integration of PyArrow + Parquet dramatically improves read/write speeds, particularly for large datasets. PyArrow also integrates seamlessly with cloud storage and big data frameworks, making it ideal for scalable, high-performance data pipelines. Nonetheless - Pandas also does feature PyArrow capabilites on the backend with the inclusion of a suitable engine - allowing for it to read files fairly quickly as well!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_arrow(df, filename=\"data/genes_filtered.parquet\"):\n",
    "    arrow_table = pa.Table.from_pandas(df) #converts to arrow table\n",
    "    pq.write_table(arrow_table, filename) #saves as parquet\n",
    "    print(f\"Data saved as Parquet: {filename}\")\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_arrow_table(filename):\n",
    "    arrow_table = pq.read_table(filename) #using the parquet file for analysis\n",
    "    print(\"Arrow Table Loaded:\")\n",
    "    print(arrow_table.schema) #maintain the same schema cuz arrow is quite good (main point of using it)\n",
    "    return arrow_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved as Parquet: data/genes_filtered.parquet\n",
      "Arrow Table Loaded:\n",
      "seqname: int64\n",
      "start: int64\n",
      "end: int64\n",
      "strand: string\n",
      "gene_id: string\n",
      "gene_name: string\n",
      "gene_biotype: string\n",
      "__index_level_0__: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [\"__index_level_0__\"], \"column_indexes\": [{\"na' + 1118\n"
     ]
    }
   ],
   "source": [
    "# Full pipeline execution: fetch from s3; parse, clean, filter and extract attributes; store in parquet\n",
    "if __name__ == \"__main__\":\n",
    "    bucket_name = \"sg-nex-data\"\n",
    "    file_key = \"data/data_tutorial/annotations/hg38_chr22.gtf\"\n",
    "\n",
    "    gtf_text = load_gtf_data(bucket_name, file_key)\n",
    "    if gtf_text:\n",
    "        gtf_df = parse_gtf_data(gtf_text)\n",
    "        filtered_genes_df = filter_genes(gtf_df)\n",
    "        processed_df = extract_attributes(filtered_genes_df)\n",
    "\n",
    "        parquet_file = convert_to_arrow(processed_df)\n",
    "\n",
    "        arrow_table = load_arrow_table(parquet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the Arrow Table:\n",
      "seqname: int64\n",
      "start: int64\n",
      "end: int64\n",
      "strand: string\n",
      "gene_id: string\n",
      "gene_name: string\n",
      "gene_biotype: string\n",
      "__index_level_0__: int64\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [\"__index_level_0__\"], \"column_indexes\": [{\"na' + 1118\n",
      "\n",
      "Column Names: ['seqname', 'start', 'end', 'strand', 'gene_id', 'gene_name', 'gene_biotype', '__index_level_0__']\n",
      "\n",
      "Metadata: {b'pandas': b'{\"index_columns\": [\"__index_level_0__\"], \"column_indexes\": [{\"name\": null, \"field_name\": null, \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": {\"encoding\": \"UTF-8\"}}], \"columns\": [{\"name\": \"seqname\", \"field_name\": \"seqname\", \"pandas_type\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}, {\"name\": \"start\", \"field_name\": \"start\", \"pandas_type\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}, {\"name\": \"end\", \"field_name\": \"end\", \"pandas_type\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}, {\"name\": \"strand\", \"field_name\": \"strand\", \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null}, {\"name\": \"gene_id\", \"field_name\": \"gene_id\", \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null}, {\"name\": \"gene_name\", \"field_name\": \"gene_name\", \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null}, {\"name\": \"gene_biotype\", \"field_name\": \"gene_biotype\", \"pandas_type\": \"unicode\", \"numpy_type\": \"object\", \"metadata\": null}, {\"name\": null, \"field_name\": \"__index_level_0__\", \"pandas_type\": \"int64\", \"numpy_type\": \"int64\", \"metadata\": null}], \"creator\": {\"library\": \"pyarrow\", \"version\": \"18.1.0\"}, \"pandas_version\": \"2.2.2\"}'}\n",
      "\n",
      "First 5 rows of the Pandas DataFrame:\n",
      "    seqname     start       end strand          gene_id   gene_name  \\\n",
      "0        22  10736171  10736283      -  ENSG00000277248          U2   \n",
      "3        22  10939388  10961338      -  ENSG00000283047  CU459211.1   \n",
      "14       22  11066418  11068174      +  ENSG00000279973  CU104787.1   \n",
      "18       22  11124337  11125705      +  ENSG00000226444    ACTR3BP6   \n",
      "22       22  11249809  11249959      -  ENSG00000276871   5_8S_rRNA   \n",
      "\n",
      "              gene_biotype  \n",
      "0                    snRNA  \n",
      "3   unprocessed_pseudogene  \n",
      "14                 lincRNA  \n",
      "18    processed_pseudogene  \n",
      "22                    rRNA  \n",
      "\n",
      "Arrow Table with Selected Columns:\n",
      "pyarrow.Table\n",
      "gene_id: string\n",
      "gene_name: string\n",
      "----\n",
      "gene_id: [[\"ENSG00000277248\",\"ENSG00000283047\",\"ENSG00000279973\",\"ENSG00000226444\",\"ENSG00000276871\",...,\"ENSG00000100312\",\"ENSG00000254499\",\"ENSG00000213683\",\"ENSG00000184319\",\"ENSG00000079974\"]]\n",
      "gene_name: [[\"U2\",\"CU459211.1\",\"CU104787.1\",\"ACTR3BP6\",\"5_8S_rRNA\",...,\"ACR\",\"AC002056.2\",\"AC002056.1\",\"RPL23AP82\",\"RABL2B\"]]\n"
     ]
    }
   ],
   "source": [
    "arrow_table = pq.read_table(\"data/genes_filtered.parquet\")\n",
    "\n",
    "print(\"Schema of the Arrow Table:\")\n",
    "print(arrow_table.schema)\n",
    "\n",
    "print(\"\\nColumn Names:\", arrow_table.column_names) #check column names\n",
    "print(\"\\nMetadata:\", arrow_table.schema.metadata) # show metadata of arrow\n",
    "\n",
    "df = arrow_table.to_pandas() # convert to pandas to compare\n",
    "print(\"\\nFirst 5 rows of the Pandas DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "selected_columns = arrow_table.select([\"gene_id\", \"gene_name\"]) #to compare selective columns\n",
    "print(\"\\nArrow Table with Selected Columns:\")\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data loaded back using Pandas (via PyArrow):\n",
      "    seqname     start       end strand          gene_id   gene_name  \\\n",
      "0        22  10736171  10736283      -  ENSG00000277248          U2   \n",
      "3        22  10939388  10961338      -  ENSG00000283047  CU459211.1   \n",
      "14       22  11066418  11068174      +  ENSG00000279973  CU104787.1   \n",
      "18       22  11124337  11125705      +  ENSG00000226444    ACTR3BP6   \n",
      "22       22  11249809  11249959      -  ENSG00000276871   5_8S_rRNA   \n",
      "\n",
      "              gene_biotype  \n",
      "0                    snRNA  \n",
      "3   unprocessed_pseudogene  \n",
      "14                 lincRNA  \n",
      "18    processed_pseudogene  \n",
      "22                    rRNA  \n"
     ]
    }
   ],
   "source": [
    "df.to_parquet(\"data/genes_pandas.parquet\", engine=\"pyarrow\") \n",
    "# save using Pandas (it uses PyArrow under the hood)\n",
    "df_pandas = pd.read_parquet(\"data/genes_pandas.parquet\", engine=\"pyarrow\") \n",
    "#loads it back\n",
    "\n",
    "print(\"\\nData loaded back using Pandas (via PyArrow):\")\n",
    "print(df_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas read time: 0.010181903839111328 seconds\n",
      "Arrow read time: 0.00826406478881836 seconds\n"
     ]
    }
   ],
   "source": [
    "# Measure Pandas read time\n",
    "start_time = time.time()\n",
    "df_pandas = pd.read_parquet(\"data/genes_pandas.parquet\", engine=\"pyarrow\")\n",
    "print(f\"Pandas read time: {time.time() - start_time} seconds\")\n",
    "\n",
    "# Measure Arrow read time\n",
    "start_time = time.time()\n",
    "arrow_table = pq.read_table(\"data/genes_pandas.parquet\")\n",
    "print(f\"Arrow read time: {time.time() - start_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
