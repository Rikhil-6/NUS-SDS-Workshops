{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import gzip\n",
    "import boto3\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Engineering**\n",
    "Data engineering is a critical process in the loading and preparing data for analysis by ensuring its accessibility, quality, and structure.\n",
    "In the modern day and age - this is especially pertinent when it comes to integrating with databases on cloud-based platforms - such as AWS!!  \n",
    "\n",
    "In this section, we focus on how boto3, the AWS Software Development Kit (SDK) for Python, facilitates integration with Amazon S3 to access, download, and process data. By leveraging AWS services, we can efficiently manage large datasets, even those stored in compressed formats like `.db.gz`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an anonymous S3 client (DISABLE SIGNATURES) --> ONLY for public datasets\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# Choice of (Public) S3 bucket (amend as you choose to!)\n",
    "bucket_name = \"megascenes\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_response = s3.list_objects_v2(Bucket=bucket_name, MaxKeys = 20)  # Fetch first 20 files\n",
    "\n",
    "# Print the available file keys --> files which we can choose to access\n",
    "if \"Contents\" in s3_response:\n",
    "    print(\"Files in the bucket:\")\n",
    "    for obj in s3_response[\"Contents\"]:\n",
    "        print(\"-\", obj[\"Key\"])\n",
    "else:\n",
    "    print(\"No files found or access denied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_bucket = list(map(lambda x:x[\"Key\"],s3.list_objects_v2(Bucket=bucket_name)[\"Contents\"])) \n",
    "# Similar as above - only this time no max limit\n",
    "print(len(db_bucket))\n",
    "db_bucket[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'databases/descriptors/000/000/descriptors.db.gz' # Amend as you wish!\n",
    "# Read the file from S3 into memory\n",
    "obj_response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "compressed_data = obj_response[\"Body\"].read()\n",
    "\n",
    "# Decompress the data\n",
    "with gzip.GzipFile(fileobj=io.BytesIO(compressed_data)) as f:\n",
    "    decompressed_data = f.read()\n",
    "\n",
    "# Write to an in-memory database\n",
    "db_buffer = io.BytesIO(decompressed_data)\n",
    "\n",
    "# Connect SQLite to the in-memory buffer - no files being downloaded yet\n",
    "conn = sqlite3.connect(\":memory:\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the decompressed database into .db file for SQLITE to connect to\n",
    "with open(\"temp.db\", \"wb\") as temp_db:\n",
    "    temp_db.write(db_buffer.getvalue())\n",
    "\n",
    "# Reconnect to the temporary SQLite database\n",
    "conn = sqlite3.connect(\"temp.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\",conn) # Get table names of those availabe in the database\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the table into pandas\n",
    "df = pd.read_sql_query(\"SELECT * FROM descriptors\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GenAI & Data Extraction**\n",
    "\n",
    "The code listed above is a pretty standard way to extract data from an AWS bucket (one that is publicly available) to one's local machine. Yet coding it out time and time again for various datasets from a number of buckets is bound to be time consuming and not supremely productive. \n",
    "\n",
    "Indeed - it would be thus preferential to have someone/ something else handle this for us as Data Engineers - and that's where AI comes in! However, the task is not truly as simple as passing in the url of interest and asking 'Chat - please read this for me, thank you' (as much as one may wish that is the case). \n",
    "\n",
    "Thus - the data engineer must use his/her prior knowledge to instruct the model clearly on what to do to produce reliable outputs as need be - and also be prepared to troubleshoot it wherever necessary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"SDS_OpenAI_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.001) # the temperature setting can be thought of as a way to set the variability of the \n",
    "# result generated - as reliable code is expected as an output - the number passed into it is fairly low to get *somewhat* consistent\n",
    "# outputs of code after a number of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Attempt to craft a specific prompt (iteratively improve it by analysing the output produced and how the code actually runs)\n",
    "#  so as to be able to extract the file from AWS onto one in a pandas DataFrame\n",
    "\n",
    "prompt = f\"\"\"Chat, given the available s3 database\n",
    "with bucket {bucket_name} and file {file_key} \n",
    "help me read the file key - give me the code in Python please.\n",
    "\"\"\"\n",
    "\n",
    "# Improve this prompt as need be!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = llm.invoke(prompt)\n",
    "print(chat_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert the code from GPT here!!!\n",
    "\n",
    "\"\"\" \n",
    "FOR AI CODE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of files from memory\n",
    "os.remove(\"temp.db\")\n",
    "os.remove(\"descriptors.db\")\n",
    "os.remove(\"descriptors.db.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GenAI & Transformation**\n",
    "\n",
    "As we have explored previously, GenAI can be a very useful tool for a data engineer to craft applicable code for the parsing of databases from cloud base sources - especially when a good prompt is used. But that is not the most value adding stage for GenAI on its own in the ETL pipeline/ in pipelining in General. \n",
    "\n",
    "Rather - GenAI can be integrated in the Transformation stage on our data, as well as on the Analytical stage - to offer insights to ways in which our data can be transformed (ideally beyond the standard ways of imputing missing values/ changing datatypes) for our benefit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prev = pd.read_csv(r\"data\\yellow_tripdata_2019-01.csv\")\n",
    "df_recent = pd.read_csv(r\"data\\yellow_tripdata_2020-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recent.sample(100) # sample of the `recent` dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 20\n",
    "prompt = f\"\"\"Chat, given the dataframes {df_prev.sample(nrows)} and\n",
    "{df_recent.sample(nrows)}, what are some things I can do with them\n",
    "\"\"\"\n",
    "# Improve the aforementioned prompt to enable GenAI to grant you some ideas to what transformations you can implement!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = llm.invoke(prompt)\n",
    "print(chat_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Improve this prompt to output the relevant code for wild/wacky tranformations that can be applied on your dataframe \n",
    "## Craft your prompt in such a way that the code granted by GPT requires limited tuning/ modification for optimal performance!!\n",
    "\n",
    "prompt = f\"\"\"Chat, given the dataframe {df_recent.sample(10)} with column names {df_recent.columns} tell me how to apply some transformations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_response = llm.invoke(prompt)\n",
    "print(chat_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Insert the code from GPT here!!!\n",
    "\n",
    "\"\"\" \n",
    "FOR AI CODE\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
