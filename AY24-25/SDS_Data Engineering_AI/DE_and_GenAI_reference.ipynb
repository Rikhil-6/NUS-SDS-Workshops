{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.config import Config\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import gzip\n",
    "import sqlite3\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Engineering**\n",
    "Data engineering is a critical process in the loading and preparing data for analysis by ensuring its accessibility, quality, and structure.\n",
    "In the modern day and age - this is especially pertinent when it comes to integrating with databases on cloud-based platforms - such as AWS!!  \n",
    "\n",
    "In this section, we focus on how boto3, the AWS Software Development Kit (SDK) for Python, facilitates integration with Amazon S3 to access, download, and process data. By leveraging AWS services, we can efficiently manage large datasets, even those stored in compressed formats like `.db.gz`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an anonymous S3 client (DISABLE SIGNATURES) --> ONLY for public datasets\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# Choice of (Public) S3 bucket (amend as you choose to!)\n",
    "bucket_name = \"megascenes\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in the bucket:\n",
      "- databases/\n",
      "- databases/descriptors/\n",
      "- databases/descriptors/000/000/descriptors.db.gz\n",
      "- databases/descriptors/000/001/descriptors.db.gz\n",
      "- databases/descriptors/000/002/descriptors.db.gz\n",
      "- databases/descriptors/000/003/descriptors.db.gz\n",
      "- databases/descriptors/000/004/descriptors.db.gz\n",
      "- databases/descriptors/000/005/descriptors.db.gz\n",
      "- databases/descriptors/000/006/descriptors.db.gz\n",
      "- databases/descriptors/000/007/descriptors.db.gz\n",
      "- databases/descriptors/000/008/descriptors.db.gz\n",
      "- databases/descriptors/000/009/descriptors.db.gz\n",
      "- databases/descriptors/000/010/descriptors.db.gz\n",
      "- databases/descriptors/000/011/descriptors.db.gz\n",
      "- databases/descriptors/000/012/descriptors.db.gz\n",
      "- databases/descriptors/000/013/descriptors.db.gz\n",
      "- databases/descriptors/000/014/descriptors.db.gz\n",
      "- databases/descriptors/000/015/descriptors.db.gz\n",
      "- databases/descriptors/000/016/descriptors.db.gz\n",
      "- databases/descriptors/000/017/descriptors.db.gz\n"
     ]
    }
   ],
   "source": [
    "s3_response = s3.list_objects_v2(Bucket=bucket_name, MaxKeys = 20)  # Fetch first 20 files\n",
    "\n",
    "# Print the available file keys --> files which we can choose to access\n",
    "if \"Contents\" in s3_response:\n",
    "    print(\"Files in the bucket:\")\n",
    "    for obj in s3_response[\"Contents\"]:\n",
    "        print(\"-\", obj[\"Key\"])\n",
    "else:\n",
    "    print(\"No files found or access denied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['databases/',\n",
       " 'databases/descriptors/',\n",
       " 'databases/descriptors/000/000/descriptors.db.gz',\n",
       " 'databases/descriptors/000/001/descriptors.db.gz',\n",
       " 'databases/descriptors/000/002/descriptors.db.gz',\n",
       " 'databases/descriptors/000/003/descriptors.db.gz',\n",
       " 'databases/descriptors/000/004/descriptors.db.gz',\n",
       " 'databases/descriptors/000/005/descriptors.db.gz',\n",
       " 'databases/descriptors/000/006/descriptors.db.gz',\n",
       " 'databases/descriptors/000/007/descriptors.db.gz',\n",
       " 'databases/descriptors/000/008/descriptors.db.gz',\n",
       " 'databases/descriptors/000/009/descriptors.db.gz',\n",
       " 'databases/descriptors/000/010/descriptors.db.gz',\n",
       " 'databases/descriptors/000/011/descriptors.db.gz',\n",
       " 'databases/descriptors/000/012/descriptors.db.gz',\n",
       " 'databases/descriptors/000/013/descriptors.db.gz',\n",
       " 'databases/descriptors/000/014/descriptors.db.gz',\n",
       " 'databases/descriptors/000/015/descriptors.db.gz',\n",
       " 'databases/descriptors/000/016/descriptors.db.gz',\n",
       " 'databases/descriptors/000/017/descriptors.db.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_bucket = list(map(lambda x:x[\"Key\"],s3.list_objects_v2(Bucket=bucket_name)[\"Contents\"])) \n",
    "# Similar as above - only this time no max limit\n",
    "print(len(db_bucket))\n",
    "db_bucket[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_key = 'databases/descriptors/000/000/descriptors.db.gz' # Amend as you wish!\n",
    "# Read the file from S3 into memory\n",
    "obj_response = s3.get_object(Bucket=bucket_name, Key=file_key)\n",
    "compressed_data = obj_response[\"Body\"].read()\n",
    "\n",
    "# Decompress the data\n",
    "with gzip.GzipFile(fileobj=io.BytesIO(compressed_data)) as f:\n",
    "    decompressed_data = f.read()\n",
    "\n",
    "# Write to an in-memory database\n",
    "db_buffer = io.BytesIO(decompressed_data)\n",
    "\n",
    "# Connect SQLite to the in-memory buffer - no files being downloaded yet\n",
    "conn = sqlite3.connect(\":memory:\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the decompressed database into .db file for SQLITE to connect to\n",
    "with open(\"temp.db\", \"wb\") as temp_db:\n",
    "    temp_db.write(db_buffer.getvalue())\n",
    "\n",
    "# Reconnect to the temporary SQLite database\n",
    "conn = sqlite3.connect(\"temp.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>descriptors</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          name\n",
       "0  descriptors"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table'\",conn) # Get table names of those availabe in the database\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the table into pandas\n",
    "df = pd.read_sql_query(\"SELECT * FROM descriptors\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GenAI & Data Extraction**\n",
    "\n",
    "The code listed above is a pretty standard way to extract data from an AWS bucket (one that is publicly available) to one's local machine. Yet coding it out time and time again for various datasets from a number of buckets is bound to be time consuming and not supremely productive. \n",
    "\n",
    "Indeed - it would be thus preferential to have someone/ something else handle this for us as Data Engineers - and that's where AI comes in! However, the task is not truly as simple as passing in the url of interest and asking 'Chat - please read this for me, thank you' (as much as one may wish that is the case). \n",
    "\n",
    "Thus - the data engineer must use his/her prior knowledge to instruct the model clearly on what to do to produce reliable outputs as need be - and also be prepared to troubleshoot it wherever necessary! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_1520\\2459014232.py:5: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.001) # the temperature setting can be thought of as a way to set the variability of the\n"
     ]
    }
   ],
   "source": [
    "with open(\"SDS_OpenAI_key.txt\", \"r\") as f:\n",
    "    api_key = f.read().strip()\n",
    "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.001) # the temperature setting can be thought of as a way to set the variability of the \n",
    "# result generated - as reliable code is expected as an output - the number passed into it is fairly low to get *somewhat* consistent\n",
    "# outputs of code after a number of runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ideal prompt\n",
    "prompt = f\"\"\"Chat, given that I am using a publicly available s3 database\n",
    "with bucket {bucket_name} and file {file_key} being parsed through boto3 \n",
    "via an s3 client with UNSIGNED credentials from botocore in Python. \n",
    "How can I read the specific file type of {file_key} into a pandas based dataframe? \n",
    "I only want my output to be in code form like ```df = pd.read_sql_query()``` and not have any human text present. \n",
    "The file may be temporarily stored locally should its size be less than 500 MB\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import pandas as pd\n",
      "import boto3\n",
      "import botocore\n",
      "from sqlalchemy import create_engine\n",
      "import gzip\n",
      "import shutil\n",
      "import os\n",
      "\n",
      "# Create an UNSIGNED S3 client\n",
      "s3 = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))\n",
      "\n",
      "bucket_name = 'megascenes'\n",
      "file_key = 'databases/descriptors/000/000/descriptors.db.gz'\n",
      "local_file_name = 'descriptors.db.gz'\n",
      "local_unzipped_file_name = 'descriptors.db'\n",
      "\n",
      "# Download the file from S3\n",
      "s3.download_file(bucket_name, file_key, local_file_name)\n",
      "\n",
      "# Unzip the file\n",
      "with gzip.open(local_file_name, 'rb') as f_in:\n",
      "    with open(local_unzipped_file_name, 'wb') as f_out:\n",
      "        shutil.copyfileobj(f_in, f_out)\n",
      "\n",
      "# Create a database connection\n",
      "engine = create_engine('sqlite:///' + local_unzipped_file_name)\n",
      "\n",
      "# Read the SQL database file into a pandas DataFrame\n",
      "df = pd.read_sql_query(\"SELECT * FROM your_table_name\", engine)\n",
      "\n",
      "# Remove the local files\n",
      "os.remove(local_file_name)\n",
      "os.remove(local_unzipped_file_name)\n",
      "```\n",
      "Please replace `\"SELECT * FROM your_table_name\"` with your actual SQL query.\n"
     ]
    }
   ],
   "source": [
    "chat_response = llm.invoke(prompt)\n",
    "print(chat_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output from ChatGPT!\n",
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import gzip\n",
    "import sqlite3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# Parse the S3 URL\n",
    "url = urlparse('s3://megascenes/databases/descriptors/000/000/descriptors.db.gz')\n",
    "\n",
    "# Create an S3 client\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "# Download the file\n",
    "with open('descriptors.db.gz', 'wb') as f:\n",
    "    s3.download_fileobj(url.netloc, url.path.lstrip('/'), f)\n",
    "\n",
    "# Unzip the file\n",
    "with gzip.open('descriptors.db.gz', 'rb') as f_in:\n",
    "    with open('descriptors.db', 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('descriptors.db')\n",
    "\n",
    "# Read the SQL query into a DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * FROM descriptors\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of files from memory\n",
    "os.remove(\"temp.db\")\n",
    "os.remove(\"descriptors.db\")\n",
    "os.remove(\"descriptors.db.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GenAI & Transformation**\n",
    "\n",
    "As we have explored previously, GenAI can be a very useful tool for a data engineer to craft applicable code for the parsing of databases from cloud base sources - especially when a good prompt is used. But that is not the most value adding stage for GenAI on its own in the ETL pipeline/ in pipelining in General. \n",
    "\n",
    "Rather - GenAI can be integrated in the Transformation stage on our data, as well as on the Analytical stage - to offer insights to ways in which our data can be transformed (ideally beyond the standard ways of imputing missing values/ changing datatypes) for our benefit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_1520\\249021592.py:2: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_recent = pd.read_csv(r\"data\\yellow_tripdata_2020-01.csv\")\n"
     ]
    }
   ],
   "source": [
    "df_prev = pd.read_csv(r\"data\\yellow_tripdata_2019-01.csv\")\n",
    "df_recent = pd.read_csv(r\"data\\yellow_tripdata_2020-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>RatecodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>congestion_surcharge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6377519</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-15 17:37:45</td>\n",
       "      <td>2020-01-15 18:09:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>231</td>\n",
       "      <td>225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.16</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>29.21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924522</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-06 12:54:11</td>\n",
       "      <td>2020-01-06 13:21:07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.70</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>132</td>\n",
       "      <td>262</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12.25</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>73.67</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122714</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-01 16:24:30</td>\n",
       "      <td>2020-01-01 16:57:45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.30</td>\n",
       "      <td>2.0</td>\n",
       "      <td>N</td>\n",
       "      <td>230</td>\n",
       "      <td>132</td>\n",
       "      <td>2.0</td>\n",
       "      <td>52.00</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.12</td>\n",
       "      <td>0.3</td>\n",
       "      <td>61.42</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564446</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-14 11:46:45</td>\n",
       "      <td>2020-01-14 12:06:04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.98</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>100</td>\n",
       "      <td>231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>20.16</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974733</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-01-16 09:23:12</td>\n",
       "      <td>2020-01-16 09:32:34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>74</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530376</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-18 17:56:37</td>\n",
       "      <td>2020-01-18 18:04:57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>12.36</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667487</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-24 11:40:57</td>\n",
       "      <td>2020-01-24 12:01:22</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>186</td>\n",
       "      <td>143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>17.30</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4804576</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-24 22:37:12</td>\n",
       "      <td>2020-01-24 22:47:01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>90</td>\n",
       "      <td>114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>13.05</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631371</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-09 20:26:13</td>\n",
       "      <td>2020-01-09 20:31:34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>211</td>\n",
       "      <td>114</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>11.16</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484516</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2020-01-28 11:20:22</td>\n",
       "      <td>2020-01-28 11:23:34</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.0</td>\n",
       "      <td>N</td>\n",
       "      <td>113</td>\n",
       "      <td>234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>8.76</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "6377519       NaN  2020-01-15 17:37:45   2020-01-15 18:09:41              NaN   \n",
       "924522        1.0  2020-01-06 12:54:11   2020-01-06 13:21:07              1.0   \n",
       "122714        1.0  2020-01-01 16:24:30   2020-01-01 16:57:45              1.0   \n",
       "2564446       2.0  2020-01-14 11:46:45   2020-01-14 12:06:04              1.0   \n",
       "2974733       1.0  2020-01-16 09:23:12   2020-01-16 09:32:34              0.0   \n",
       "...           ...                  ...                   ...              ...   \n",
       "3530376       2.0  2020-01-18 17:56:37   2020-01-18 18:04:57              1.0   \n",
       "4667487       2.0  2020-01-24 11:40:57   2020-01-24 12:01:22              1.0   \n",
       "4804576       2.0  2020-01-24 22:37:12   2020-01-24 22:47:01              1.0   \n",
       "1631371       2.0  2020-01-09 20:26:13   2020-01-09 20:31:34              1.0   \n",
       "5484516       2.0  2020-01-28 11:20:22   2020-01-28 11:23:34              1.0   \n",
       "\n",
       "         trip_distance  RatecodeID store_and_fwd_flag  PULocationID  \\\n",
       "6377519           5.94         NaN                NaN           231   \n",
       "924522           19.70         2.0                  N           132   \n",
       "122714           18.30         2.0                  N           230   \n",
       "2564446           2.98         1.0                  N           100   \n",
       "2974733           1.10         1.0                  N            74   \n",
       "...                ...         ...                ...           ...   \n",
       "3530376           1.19         1.0                  N           144   \n",
       "4667487           2.35         1.0                  N           186   \n",
       "4804576           1.28         1.0                  N            90   \n",
       "1631371           0.73         1.0                  N           211   \n",
       "5484516           0.60         1.0                  N           113   \n",
       "\n",
       "         DOLocationID  payment_type  fare_amount  extra  mta_tax  tip_amount  \\\n",
       "6377519           225           NaN        26.16   2.75      0.0        0.00   \n",
       "924522            262           1.0        52.00   2.50      0.5       12.25   \n",
       "122714            132           2.0        52.00   2.50      0.5        0.00   \n",
       "2564446           231           1.0        13.50   0.00      0.5        3.36   \n",
       "2974733            41           1.0         7.50   0.00      0.5        0.70   \n",
       "...               ...           ...          ...    ...      ...         ...   \n",
       "3530376             4           1.0         7.00   0.00      0.5        2.06   \n",
       "4667487           143           2.0        14.00   0.00      0.5        0.00   \n",
       "4804576           114           1.0         8.00   0.50      0.5        1.25   \n",
       "1631371           114           1.0         5.50   0.50      0.5        1.86   \n",
       "5484516           234           1.0         4.00   0.00      0.5        1.46   \n",
       "\n",
       "         tolls_amount  improvement_surcharge  total_amount  \\\n",
       "6377519          0.00                    0.3         29.21   \n",
       "924522           6.12                    0.3         73.67   \n",
       "122714           6.12                    0.3         61.42   \n",
       "2564446          0.00                    0.3         20.16   \n",
       "2974733          0.00                    0.3          9.00   \n",
       "...               ...                    ...           ...   \n",
       "3530376          0.00                    0.3         12.36   \n",
       "4667487          0.00                    0.3         17.30   \n",
       "4804576          0.00                    0.3         13.05   \n",
       "1631371          0.00                    0.3         11.16   \n",
       "5484516          0.00                    0.3          8.76   \n",
       "\n",
       "         congestion_surcharge  \n",
       "6377519                   0.0  \n",
       "924522                    2.5  \n",
       "122714                    2.5  \n",
       "2564446                   2.5  \n",
       "2974733                   0.0  \n",
       "...                       ...  \n",
       "3530376                   2.5  \n",
       "4667487                   2.5  \n",
       "4804576                   2.5  \n",
       "1631371                   2.5  \n",
       "5484516                   2.5  \n",
       "\n",
       "[100 rows x 18 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_recent.sample(100) # sample of the `recent` dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 20\n",
    "\n",
    "## Ideal prompt\n",
    "prompt = f\"\"\"Chat, given that I have 2 dataframes {df_prev.sample(nrows)} collected in 2019 and\n",
    "{df_recent.sample(nrows)} collected in 2020 - identify the relationships between these dataframes - their differences\n",
    "and also possible transformations that can be applied to the relevant columns in python based code. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two dataframes seem to be taxi trip data collected in the years 2019 and 2020 respectively. They have the same structure and columns, which include details about the trip such as pickup and dropoff times, locations, distance, fare, and payment details.\n",
      "\n",
      "Differences:\n",
      "1. The data is collected in different years (2019 and 2020).\n",
      "2. The actual data in the rows will be different as they represent different taxi trips.\n",
      "3. There might be differences in the data quality or completeness between the two years. For example, the 'congestion_surcharge' column in the 2019 dataframe has some missing values (NaN), while it seems to be complete in the 2020 dataframe.\n",
      "\n",
      "Possible Transformations:\n",
      "1. Date-Time Conversion: The 'tpep_pickup_datetime' and 'tpep_dropoff_datetime' columns are currently in string format. They can be converted to datetime format for further analysis. This can be done using pandas to_datetime function.\n",
      "   ```\n",
      "   df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
      "   df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
      "   ```\n",
      "2. Duration Calculation: A new column 'trip_duration' can be created by subtracting the pickup time from the dropoff time.\n",
      "   ```\n",
      "   df['trip_duration'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
      "   ```\n",
      "3. Handling Missing Values: The missing values in the 'congestion_surcharge' column can be filled with appropriate values, possibly 0 or the median of the column.\n",
      "   ```\n",
      "   df['congestion_surcharge'].fillna(df['congestion_surcharge'].median(), inplace=True)\n",
      "   ```\n",
      "4. Categorical Encoding: The 'store_and_fwd_flag' column is a categorical variable with 'Y' and 'N' values. It can be encoded to numerical values (1 and 0) for use in machine learning models.\n",
      "   ```\n",
      "   df['store_and_fwd_flag'] = df['store_and_fwd_flag'].map({'Y': 1, 'N': 0})\n",
      "   ```\n",
      "5. Outlier Detection and Removal: Outliers in columns like 'trip_distance', 'fare_amount', 'tip_amount', etc. can be detected and removed for more accurate analysis.\n",
      "6. Feature Engineering: New features can be created based on existing columns. For example, 'hour_of_day', 'day_of_week', 'is_weekend' etc. can be derived from the pickup datetime.\n"
     ]
    }
   ],
   "source": [
    "chat_response = llm.invoke(prompt)\n",
    "print(chat_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Chat, given the dataframe {df_recent.sample(10)} - outline useful transformations that can be applied to \n",
    "the data to make it robust to new possible entries that can be received in the future - \n",
    "do also give the relevant python code and libraries required to achieve these tasks. \n",
    "Note that the column names are {df_recent.columns}.\n",
    "Ensure these are NOT simplistic transformations like standardisation/ missing value imputation.\n",
    "Give me at least 15 distinct transformations - with the relevant code being present. Analyse the dataset itself to build deep \n",
    "and useful transformations that can be mapped onto it - including the creation of new columns\n",
    "\"\"\"\n",
    "\n",
    "## Ideal prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. **Datetime Conversion**: Convert 'tpep_pickup_datetime' and 'tpep_dropoff_datetime' from string to datetime format. This will allow for more complex time-based analysis and feature engineering.\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
      "df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
      "```\n",
      "\n",
      "2. **Trip Duration**: Calculate the duration of each trip in minutes. This could be a useful feature for predicting fare amounts or tip amounts.\n",
      "\n",
      "```python\n",
      "df['trip_duration'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
      "```\n",
      "\n",
      "3. **Hour of Day**: Extract the hour of day from the pickup and dropoff times. This could be useful for understanding patterns related to time of day.\n",
      "\n",
      "```python\n",
      "df['pickup_hour'] = df['tpep_pickup_datetime'].dt.hour\n",
      "df['dropoff_hour'] = df['tpep_dropoff_datetime'].dt.hour\n",
      "```\n",
      "\n",
      "4. **Day of Week**: Extract the day of the week from the pickup and dropoff times. This could be useful for understanding patterns related to day of the week.\n",
      "\n",
      "```python\n",
      "df['pickup_day_of_week'] = df['tpep_pickup_datetime'].dt.dayofweek\n",
      "df['dropoff_day_of_week'] = df['tpep_dropoff_datetime'].dt.dayofweek\n",
      "```\n",
      "\n",
      "5. **Weekend Flag**: Create a binary flag indicating whether the trip occurred on a weekend. This could be useful for understanding patterns related to weekends vs. weekdays.\n",
      "\n",
      "```python\n",
      "df['pickup_weekend'] = df['pickup_day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
      "df['dropoff_weekend'] = df['dropoff_day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
      "```\n",
      "\n",
      "6. **Rush Hour Flag**: Create a binary flag indicating whether the trip occurred during rush hour. This could be useful for understanding patterns related to traffic congestion.\n",
      "\n",
      "```python\n",
      "df['pickup_rush_hour'] = df['pickup_hour'].apply(lambda x: 1 if 7 <= x <= 9 or 16 <= x <= 18 else 0)\n",
      "df['dropoff_rush_hour'] = df['dropoff_hour'].apply(lambda x: 1 if 7 <= x <= 9 or 16 <= x <= 18 else 0)\n",
      "```\n",
      "\n",
      "7. **Distance per Minute**: Calculate the trip distance per minute. This could be a useful feature for understanding the speed of the trip.\n",
      "\n",
      "```python\n",
      "df['distance_per_minute'] = df['trip_distance'] / df['trip_duration']\n",
      "```\n",
      "\n",
      "8. **Fare per Minute**: Calculate the fare amount per minute. This could be a useful feature for understanding the cost efficiency of the trip.\n",
      "\n",
      "```python\n",
      "df['fare_per_minute'] = df['fare_amount'] / df['trip_duration']\n",
      "```\n",
      "\n",
      "9. **Fare per Mile**: Calculate the fare amount per mile. This could be a useful feature for understanding the cost efficiency of the trip.\n",
      "\n",
      "```python\n",
      "df['fare_per_mile'] = df['fare_amount'] / df['trip_distance']\n",
      "```\n",
      "\n",
      "10. **Tip Percentage**: Calculate the tip amount as a percentage of the fare amount. This could be a useful feature for understanding tipping behavior.\n",
      "\n",
      "```python\n",
      "df['tip_percentage'] = df['tip_amount'] / df['fare_amount'] * 100\n",
      "```\n",
      "\n",
      "11. **Total Charge**: Calculate the total charge for the trip, including fare, extra charges, taxes, and tips. This could be a useful feature for understanding the total cost of the trip.\n",
      "\n",
      "```python\n",
      "df['total_charge'] = df[['fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge']].sum(axis=1)\n",
      "```\n",
      "\n",
      "12. **Payment Type Encoding**: Convert the 'payment_type' column to a categorical type and then use one-hot encoding. This will make the feature more suitable for machine learning algorithms.\n",
      "\n",
      "```python\n",
      "df['payment_type'] = df['payment_type'].astype('category')\n",
      "df = pd.get_dummies(df, columns=['payment_type'])\n",
      "```\n",
      "\n",
      "13. **VendorID Encoding**: Convert the 'VendorID' column to a categorical type and then use one-hot encoding. This will make the feature more suitable for machine learning algorithms.\n",
      "\n",
      "```python\n",
      "df['VendorID'] = df['VendorID'].astype('category')\n",
      "df = pd.get_dummies(df, columns=['VendorID'])\n",
      "```\n",
      "\n",
      "14. **RatecodeID Encoding**: Convert the 'RatecodeID' column to a categorical type and then use one-hot encoding. This will make the feature more suitable for machine learning algorithms.\n",
      "\n",
      "```python\n",
      "df['RatecodeID'] = df['RatecodeID'].astype('category')\n",
      "df = pd.get_dummies(df, columns=['RatecodeID'])\n",
      "```\n",
      "\n",
      "15. **Store and Forward Flag Encoding**: Convert the 'store_and_fwd_flag' column to a binary flag. This will make the feature more suitable for machine learning algorithms.\n",
      "\n",
      "```python\n",
      "df['store_and_fwd_flag'] = df['store_and_fwd_flag'].map({'N': 0, 'Y': 1})\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "chat_response = llm.invoke(prompt)\n",
    "print(chat_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output from ChatGPT!\n",
    "\n",
    "# **Datetime Conversion**: Convert 'tpep_pickup_datetime' and 'tpep_dropoff_datetime' from string to datetime format. \n",
    "# This will allow for more complex time-based analysis and feature engineering\n",
    "\n",
    "df_recent['tpep_pickup_datetime'] = pd.to_datetime(df_recent['tpep_pickup_datetime'])\n",
    "df_recent['tpep_dropoff_datetime'] = pd.to_datetime(df_recent['tpep_dropoff_datetime'])\n",
    "\n",
    "# **Day of Week**: Extract the day of the week from the pickup datetime. This could be useful for understanding patterns in taxi usage.\n",
    "\n",
    "df_recent['pickup_dayofweek'] = df_recent['tpep_pickup_datetime'].dt.dayofweek\n",
    "\n",
    "# **Is Weekend**: Determine if the pickup or dropoff is on a weekend.\n",
    "\n",
    "df_recent['pickup_is_weekend'] = df_recent['pickup_dayofweek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# **Hour of Day**: Extract the hour of the day from the pickup datetime. This could be useful for understanding patterns in taxi usage.\n",
    "\n",
    "df_recent['pickup_hour_of_day'] = df_recent['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "# **Rush Hour**: Determine if the pickup or dropoff is during rush hour (typically 7-9 AM and 4-6 PM).\n",
    "\n",
    "df_recent['pickup_rush_hour'] = df_recent['pickup_hour_of_day'].apply(lambda x: 1 if 7 <= x <= 9 or 16 <= x <= 18 else 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
