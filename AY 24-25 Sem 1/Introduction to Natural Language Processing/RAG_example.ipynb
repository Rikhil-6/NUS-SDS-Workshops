{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzu5b9bJiEtG"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) from scratch:\n",
        "## A comparison between a RAG powered chatbot and normal Chatbot\n",
        "\n",
        "#### Start of Naive RAG process:\n",
        "\n",
        "- Chunking\n",
        "- Dense Embedding of text.\n",
        "- Makeshift storing of embeddings in a dictionary (use this as a vector store python class)\n",
        "- With same query, embed it.\n",
        "- Show retrieval from makeshift vector store using cosine similarity between query vector and vectors in vector store\n",
        "- Add context to the chatbot (tell it to refer to the context provided)\n",
        "- Show the comparison between raw LLM response and RAG pipeline.\n",
        "\n",
        "#### Additional/ Advanced steps to enhance your RAG pipeline:\n",
        "\n",
        "- Query re-writing\n",
        "- Re ranking using cross encoder\n",
        "- Dynamic Embedding model fine-tuning\n",
        "- Hybrid Search using BM25/ TF-IDF\n",
        "- LLM Guardrails/ Query intention\n",
        "\n",
        "#### Some practical applications:\n",
        "\n",
        "- RAG for your school notes (eg modules like HSI, where control F (in this case vector search) could help greatly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZPOCsiZvfWU"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install groq\n",
        "\n",
        "# numpy needed for vector operations\n",
        "import numpy as np\n",
        "# regex libary needed for delimiter parsing furing text splitting\n",
        "import re\n",
        "# sentence_transformers library needed for loading our embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# groq library for our \"chatgpt\"\n",
        "from groq import Groq\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dloB3T-XiMsK"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY       = \"input-your-api-key-here\"\n",
        "CHAT_MODEL         = \"llama3-70b-8192\"\n",
        "EMBEDDING_MODEL    = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "RAG_SYSTEM_PROPMT  = \\\n",
        "'''You are a helpful, very cheerful and very bubbly assistant who only answers based on the contextual information provided, and nothing else.\n",
        "If you are unsure, say that you are unsure due to a lack of information.'''\n",
        "\n",
        "CHAT_SYSTEM_PROMPT = \\\n",
        "'''You are a helpful assistant who answers question factually. Refer very closely and analyse the context provided very carefully before answering the question.\n",
        "Hoeever, if you are unsure, say that you are unsure due to a lack of information.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U_-kpD-NiEtM"
      },
      "outputs": [],
      "source": [
        "def load_knowledge(path):\n",
        "    with open(path, 'r', encoding='utf-8') as file:\n",
        "        file_contents = file.read()\n",
        "    return file_contents\n",
        "\n",
        "class VectorStore:\n",
        "    '''Our Vector store that stores the contextual knowledge and their corresponding vector embeddings.'''\n",
        "    def __init__(self):\n",
        "        self.vectors = []\n",
        "        self.texts = []\n",
        "\n",
        "    def upsert(self, embedding, text):\n",
        "        self.vectors.extend(embedding)\n",
        "        self.texts.extend(text)\n",
        "\n",
        "    def pretty_print_contexts(self, top_k_results):\n",
        "        print(f\"{'Text':<50} {'Cosine Similarity Score':<25}\")\n",
        "        print(\"=\"*95)\n",
        "\n",
        "        # loop over each result and print it formatted\n",
        "        for result in top_k_results:\n",
        "            text = result[\"Text\"]\n",
        "            similarity_score = f\"{result['Cosine Similarity Score']:.4f}\"  # Format score to 4 decimal places\n",
        "\n",
        "            # Print each result with proper formatting\n",
        "            print(f\"{text:<50} {similarity_score:<25}\")\n",
        "\n",
        "    def retrieve(self, query_embedding, print_contexts, top_k=6):\n",
        "        '''This method is responsible for retrieving the top k vectors from the vector store'''\n",
        "        # np.dot() handles the matrix multiplication and computes the dot product between each vector in self.vectors and query_embedding\n",
        "        dot_products = np.dot(vector_store.vectors, query_embedding)\n",
        "\n",
        "        # Calculating the normalised vector of the query embedding\n",
        "        normalised_query_embedding = np.linalg.norm(query_embedding)\n",
        "\n",
        "        # Calculating the normalised vectors of each vector in vector store, is a 2D array\n",
        "        normalised_vector_embeddings = np.linalg.norm(self.vectors, axis=1)\n",
        "\n",
        "        # Calculate cosine similarity for each vector\n",
        "        cosine_similarities = dot_products / (normalised_query_embedding * normalised_vector_embeddings)\n",
        "\n",
        "        # using np.argsort to sort the similar vectors by their index\n",
        "        sorted_indices = np.argsort(cosine_similarities)\n",
        "\n",
        "        # Get indices of the top 3 most similar vectors\n",
        "        top_k_similar_indices = sorted_indices[-top_k:]\n",
        "\n",
        "        top_k_results = []\n",
        "        for i in range(len(top_k_similar_indices)):\n",
        "            similar_vector   = self.vectors[top_k_similar_indices[i]]\n",
        "            similar_text     = self.texts[i]\n",
        "            similarity_score = cosine_similarities[i]\n",
        "\n",
        "            top_k_results.append({\"Vector\": similar_vector,\n",
        "                                  \"Text\": similar_text,\n",
        "                                  \"Cosine Similarity Score\": similarity_score}\n",
        "                                )\n",
        "        if print_contexts:\n",
        "            self.pretty_print_contexts(top_k_results)\n",
        "\n",
        "        return sorted(top_k_results, reverse=True, key=lambda x: x['Cosine Similarity Score'])\n",
        "\n",
        "\n",
        "    def filter_similar_texts(self, retrieval_results):\n",
        "        similar_contexts = ''''''\n",
        "\n",
        "        # Loop through the top k results and append the texts\n",
        "        for result in retrieval_results:\n",
        "            similar_contexts += result[\"Text\"]+\"\\n\\n\"\n",
        "\n",
        "        return similar_contexts\n",
        "\n",
        "\n",
        "class TextSplitter():\n",
        "    '''Our object that enables us to split the texts according to how we want it to split.'''\n",
        "    def __init__(self, delimiters=None):\n",
        "        if delimiters is None:\n",
        "            # Use \\n\\n as delimiters for splitting if a delimiter is not specified by the user\n",
        "            self.delimiters = \"\\n\\n\"\n",
        "        else:\n",
        "            self.delimiters = delimiters\n",
        "\n",
        "    def split_text(self, text):\n",
        "        '''This method takes in text and splits the text according to the delimiters specified'''\n",
        "        # Here we are splitting by paragraphs\n",
        "        chunks = text.split(self.delimiters)\n",
        "\n",
        "        stripped_chunks =  []\n",
        "        for chunk in chunks:\n",
        "            cleaned_chunk = chunk.strip()\n",
        "            # If this chunk is not simply an empty string\n",
        "            if cleaned_chunk:\n",
        "                stripped_chunks.append(chunk)\n",
        "\n",
        "        return stripped_chunks\n",
        "\n",
        "\n",
        "class EmbeddingModel():\n",
        "    '''Our embedding model we will use for the encoding texts in the RAG pipeline'''\n",
        "    def __init__(self, model_name):\n",
        "        self.embeddings = []\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def generate_embeddings(self, texts: list):\n",
        "        # generate embeddings of all the texts, no for loop is necessary due to parallelization feature\n",
        "        embeddings = self.model.encode(texts)\n",
        "        return embeddings\n",
        "\n",
        "class chatbot():\n",
        "    '''This class is responsible for all chatbot related functions'''\n",
        "    def __init__(self):\n",
        "        self.client = Groq(api_key = GROQ_API_KEY)\n",
        "\n",
        "    def build_prompt(self, user_query, context):\n",
        "        added_sentence = '''After providing the answer, you are to reference word for word the sentece or paragraph that you referred to for answering the question.\n",
        "        '''\n",
        "        prompt = f\"Using the context provided, answer the question.\\n\\nContext:{context}\\n\\nQuestion:{user_query}\\n\\n{added_sentence}\"\n",
        "        return prompt\n",
        "\n",
        "    def fetch_response(self, prompt, system_prompt):\n",
        "        stream = self.client.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[\n",
        "                {\n",
        "                    # initialise system prompt\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": system_prompt\n",
        "                },\n",
        "                {\n",
        "                    # initialise users prompt\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,  # Control the randomness of the output (lower means less random)\n",
        "            max_tokens=1024,  # Limit the response length\n",
        "            top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
        "            stream=True,  # Enable streaming of the response chunks\n",
        "            stop=None\n",
        "        )\n",
        "\n",
        "        print(\"ChatGPT: \", end=\"\")\n",
        "        for chunk in stream:\n",
        "            print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "\n",
        "    def chat(self):\n",
        "        print(\"Welcome to ChatGPT! Feel free to ask me anything\\nTo exit, simply type 'exit' or 'quit'\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        while True:\n",
        "            query = input('User: ')\n",
        "            if query.lower() in ['exit', 'quit']:\n",
        "                print(\"Thank you for chatting!\\n\")\n",
        "                break\n",
        "            chatgpt.fetch_response(query, CHAT_SYSTEM_PROMPT)\n",
        "            print()\n",
        "        return\n",
        "\n",
        "\n",
        "    def rag_pipeline(self, contextual_knowledge):\n",
        "        # split the text accorinding to delimiters\n",
        "        text_chunks = text_splitter.split_text(contextual_knowledge)\n",
        "        # generate embeddings of desired text\n",
        "        text_embeddings = embedding_model.generate_embeddings(text_chunks)\n",
        "        # upsert the vector embeddings and text chunks to the vector store\n",
        "        vector_store.upsert(text_embeddings, text_chunks)\n",
        "\n",
        "        print(\"Welcome to the SDS Story chatbot! Feel free to ask me anything about the SDS workshop members!\\nTo exit, simply type 'exit' or 'quit'\")\n",
        "        time.sleep(1)\n",
        "        while True:\n",
        "            # get users input\n",
        "            query = input('User: ')\n",
        "\n",
        "            if query.lower() in ['exit', 'quit']:\n",
        "                print(\"Thank you for chatting!\")\n",
        "                break\n",
        "\n",
        "            # generate the vector emebeddings of the users query\n",
        "            query_embedding = embedding_model.generate_embeddings([query])[0]\n",
        "            # conduct the similarity search within the vector store\n",
        "            retrieved_results = vector_store.retrieve(query_embedding, print_contexts=False)\n",
        "            # filter out the similar texts from the retrieved results\n",
        "            contexts_to_llm = vector_store.filter_similar_texts(retrieved_results)\n",
        "            # build the prompt to include the context retrieved\n",
        "            prompt = chatgpt.build_prompt(query, contexts_to_llm)\n",
        "            chatgpt.fetch_response(prompt, RAG_SYSTEM_PROPMT)\n",
        "            print(\"================================\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ceQq4Lvat8AR"
      },
      "outputs": [],
      "source": [
        "# read in contextual_knowledge from \"database\"\n",
        "knowledge_path = 'story.txt'\n",
        "contextual_knowledge = load_knowledge(knowledge_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhHrEr4Piw4B"
      },
      "outputs": [],
      "source": [
        "vector_store    = VectorStore()\n",
        "text_splitter   = TextSplitter()\n",
        "embedding_model = EmbeddingModel(model_name= EMBEDDING_MODEL)\n",
        "chatgpt         = chatbot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9w1SR87t8AR"
      },
      "source": [
        "### Whether the rag pipeline runs or the normal chat function runs depends on the users needs and query!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci78UMZVuJfu"
      },
      "source": [
        "##  Raw ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ESlNPLwt8AR"
      },
      "outputs": [],
      "source": [
        "# Chatbot function without RAG, you can ask it anything!\n",
        "chatgpt.chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2a7U2JDuJfu"
      },
      "source": [
        "## Chatbot function with RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBVqVUWh4vVp"
      },
      "outputs": [],
      "source": [
        "# Some questions u can ask to test it out!\n",
        "\n",
        "# Who is the heart of the group?\n",
        "# What does kai wen look like?\n",
        "# What Does jerry look like?\n",
        "# What is the name of the university society mentioned in the story?\n",
        "# What project did the group take on for their first major task?\n",
        "# Which member is known for their love of optimization?\n",
        "# What was the headquarters of the Statistics and Data Science Society (SDSS) described as?\n",
        "# Who is described as the heart of the group?\n",
        "# Which member of the group had a passion for optimization?\n",
        "# What did Jerry propose during one of the SDSS meetings?\n",
        "# Who is known for breaking boundaries and challenging the rules of mathematics or coding?\n",
        "# What kind of project did the group work on that involved analyzing climate change data?\n",
        "# Who is the member of the group with a talent for data visualization?\n",
        "# What analogy does Jerry use when describing a dataset?\n",
        "# How did Gangjoon contribute to the university-wide competition on predictive analytics?\n",
        "# What was Eugene's role in making the SDSS's first major project on urban planning a success?\n",
        "# How did Kaiwen make climate change data more relatable to people?\n",
        "# What was the goal of the Predictive Human Index (PHI) project the group worked on?\n",
        "# How did Gangjoon contribute to the Predictive Human Index (PHI) project?\n",
        "# What was Jerry's long-term dream regarding the use of statistics and machine learning?\n",
        "# What type of data did Gangjoon suggest pulling for the energy consumption forecasting model in the university competition?\n",
        "\n",
        "\n",
        "chatgpt.rag_pipeline(contextual_knowledge)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
