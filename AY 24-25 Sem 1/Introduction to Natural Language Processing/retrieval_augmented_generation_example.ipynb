{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzu5b9bJiEtG"
      },
      "source": [
        "# Comparing between RAG-chatbot and non RAG Chatbot\n",
        "\n",
        "- Come up with a story about the 4 of us in SDS.\n",
        "- Question the chatbot about the story about SDS (Tell it not to make up information)\n",
        "- Show how the RAG pipeline provides contextual background information to the LLM.\n",
        "\n",
        "Start of Naive RAG process:\n",
        "\n",
        "- Chunking\n",
        "- Dense Embedding of text.\n",
        "- Makeshift storing of embeddings in a dictionary (use this as a vector store python class)\n",
        "- With same query, embed it.\n",
        "- Show retrieval from makeshift vector store using cosine similarity between query vector and vectors in vector store\n",
        "- Add context to the chatbot (tell it to refer to the context provided)\n",
        "- Show the comparison between raw LLM response and RAG pipeline.\n",
        "\n",
        "Additional/ Advanced steps to enhance your RAG pipeline:\n",
        "\n",
        "- Query re-writing\n",
        "- Re ranking using cross encoder\n",
        "- Dynamic Embedding model fine-tuning\n",
        "- Hybrid Search using BM25/ TF-IDF\n",
        "- LLM Guardrails/ Query intention\n",
        "\n",
        "Some practical applications:\n",
        "\n",
        "- RAG for your school notes (eg modules like HSI, where control F (in this case vector search) could help greatly)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install groq\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from groq import Groq\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZPOCsiZvfWU",
        "outputId": "f90f7cfc-2fd1-4520-f452-e1e5fc3e948f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GROQ_API_KEY= userdata.get('GROQ_API_KEY')\n",
        "CHAT_MODEL = \"llama3-70b-8192\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "RAG_SYSTEM_PROPMT = \"You are a helpful, very cheerful and very bubbbly assistant who only answers based on the contextual information provided, and nothing else. If you are unsure, say that you are unsure due to a lack of information.\"\n",
        "CHAT_SYSTEM_PROMPT = \"You are a helpful assistant who answers question factually. If you are unsure, say that you are unsure due to a lack of information.\"\n"
      ],
      "metadata": {
        "id": "dloB3T-XiMsK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "OGTXoPWDiEtK"
      },
      "outputs": [],
      "source": [
        "story = '''\n",
        "In a tucked-away corner of the university, beyond the buzzing of classrooms and the hum of campus life, stood the headquarters of the **Statistics and Data Science Society (SDSS)**. It wasn’t much of a headquarters, really—just a dimly lit room cluttered with whiteboards, old textbooks, tangled wires, and laptops that emitted a soft glow day and night. Yet, to its members, it was home. A sanctuary for thinkers, dreamers, and problem-solvers alike.\n",
        "\n",
        "Inside this hallowed space was a group of friends who had grown inseparable through their shared love of mathematics and coding: **Jerry**, **Eugene**, **Gangjoon**, and **Kaiwen**. Each brought their unique flavor to the society, contributing to a beautiful symphony of algorithms, puzzles, and data that made their bond unbreakable.\n",
        "\n",
        "Jerry was the heart of the group. Tall and lanky, with wild hair that seemed to mirror the whirlwind of thoughts constantly running through his mind, he was always lost in the world of numbers. But Jerry wasn’t just about solving problems—he was about finding meaning behind the data, seeing patterns where others saw randomness, and weaving stories from statistics.\n",
        "\n",
        "He often spoke in metaphors, much to the amusement of his friends. He'd glance at a dataset and say, \"This is like a river, flowing unpredictably but with purpose.\" His mind was a curious blend of abstract philosophy and cold, hard logic, and the two coexisted in perfect harmony. Jerry’s dream was to use statistics and machine learning to answer life's big questions—Why are we here? What drives human behavior? Can we predict the future?\n",
        "\n",
        "One day, as the SDSS members gathered for their usual post-lecture meeting, Jerry arrived with a wild glint in his eye. He had an idea—a radical one. \"What if we could create an algorithm that predicts human emotions based on seemingly unrelated data?\" he proposed, scribbling furiously on the whiteboard. \"Like, imagine a system that reads social media posts, stock market fluctuations, weather patterns, and even the phases of the moon, and then predicts how people will feel tomorrow.\"\n",
        "\n",
        "The room fell silent as the others tried to wrap their heads around the concept. But that was Jerry—always reaching for the stars.\n",
        "\n",
        "Where Jerry was a dreamer, Eugene was the anchor. He kept the group grounded with his practicality and level-headedness. Medium height, sharp-dressed, and always with a cup of black coffee in hand, Eugene was the one who looked at Jerry’s outlandish ideas and said, \"Okay, but how would that work in the real world?\"\n",
        "\n",
        "Eugene had a deep understanding of both mathematics and programming. He could break down even the most complex theories into manageable, bite-sized chunks, often simplifying them in ways that made the rest of the team wonder why they hadn’t thought of it first. But despite his rational demeanor, Eugene had his own flair—he was obsessed with optimization. Whether it was reducing the time complexity of an algorithm or finding the most efficient way to organize his desk, Eugene always sought perfection.\n",
        "\n",
        "His love for optimization reached a new level when the group took on their first major project: a data-driven urban planning model for the city’s transport system. The goal was to reduce traffic congestion using statistical models and simulations. While Jerry and the others brainstormed grand, creative ideas, Eugene quietly worked in the background, refining their code, improving efficiency, and making sure everything ran smoothly. When the project finally came together, it was Eugene’s touch that made it a success, and the team celebrated with a dinner that he, of course, insisted on planning down to the minute.\n",
        "\n",
        "“Data is beautiful,” he would say, “but it’s also messy. My job is to make it clean and efficient.”\n",
        "\n",
        "Gangjoon was the wild card of the group. With his unkempt hair, black hoodie, and rebellious attitude, he lived for the thrill of breaking boundaries. If there was a rule in mathematics or coding, Gangjoon’s first instinct was to challenge it. He wasn’t satisfied with conventional methods—he always wanted to push the envelope, finding loopholes in algorithms and writing code that others deemed impossible.\n",
        "\n",
        "A brilliant hacker, Gangjoon often participated in coding competitions, not for the recognition, but for the rush of outsmarting the system. He once built an encryption algorithm so complex that even the university’s computer science professor struggled to understand it. \"I don’t play by the rules,\" Gangjoon would smirk, his fingers flying across the keyboard as he conjured up yet another solution that defied logic.\n",
        "\n",
        "Despite his edgy persona, Gangjoon had a deep respect for his friends and their shared love of learning. When Jerry proposed his emotion-predicting algorithm, Gangjoon was the first to jump on board. \"Let’s hack the universe,\" he said with a grin, already thinking about ways to pull data from obscure sources and connect it in unexpected ways.\n",
        "\n",
        "But his most daring feat came when the group tackled a university-wide competition on predictive analytics. The task was to create a model that could accurately forecast energy consumption based on various factors. While others stuck to traditional datasets, Gangjoon had a radical idea: \"Why not pull data from social media activity? People’s behavior online might correlate with how much energy they use.\"\n",
        "\n",
        "It was unconventional, risky, and against the rules. But it worked. The team won the competition, thanks to Gangjoon’s audacious approach.\n",
        "\n",
        "In contrast to Gangjoon’s rebelliousness, Kaiwen was the quiet, steady presence in the group. Petite and soft-spoken, with an aura of calm about her, she often went unnoticed in large crowds. But those who knew her understood that behind her serene exterior was a mind that worked faster than any computer.\n",
        "\n",
        "Kaiwen had a talent for seeing the connections that others missed. She was the one who could take Jerry’s wild ideas, Eugene’s structured approach, and Gangjoon’s chaotic energy and blend them into something cohesive. Her specialty was in data visualization—turning raw numbers into something beautiful and understandable. Her graphs were more than just charts—they were art.\n",
        "\n",
        "When the group was working on a project to analyze climate change data, it was Kaiwen who found a way to present the information in a way that spoke to people on an emotional level. \"Data doesn’t have to be cold and impersonal,\" she said softly, sketching out a concept for an interactive map that showed the real-time impact of climate change on different regions. \"We can make it relatable, make people feel the urgency.\"\n",
        "\n",
        "Her map, when completed, became a sensation, attracting attention from professors and even local environmental organizations. Kaiwen’s ability to translate the complex into the simple, and the simple into the profound, was her superpower.\n",
        "\n",
        "As the final year of university approached, the SDSS decided to embark on their most ambitious project yet—a **Predictive Human Index (PHI)**. Jerry’s grand vision of predicting emotions had evolved over time, and now the group had a tangible goal: to create a model that could analyze a person’s data footprint and predict not only their future actions but also their emotional state and overall well-being.\n",
        "\n",
        "It was a project that required all of their strengths. Jerry provided the visionary framework, outlining the philosophical implications of predictive models on human behavior. Eugene focused on making the model scalable and functional in the real world, refining every part of the algorithm until it was a masterpiece of efficiency. Gangjoon, ever the hacker, found creative ways to pull in data from unconventional sources—everything from public forums to obscure government records. And Kaiwen, as always, turned the raw data into something visually compelling, making sure that the predictions were presented in a way that even non-experts could understand.\n",
        "\n",
        "As they worked late into the night, fueled by endless cups of coffee and the occasional slice of pizza, they felt a sense of purpose like never before. This project was more than just an academic exercise—it was their legacy.\n",
        "\n",
        "When they finally unveiled the PHI at the university’s data science symposium, the room fell silent. The model worked. It wasn’t perfect, of course, but it was a glimpse into the future of data science and human prediction. The audience, a mix of students, professors, and tech industry leaders, erupted into applause.\n",
        "\n",
        "For the SDSS, this wasn’t the end—it was just the beginning. They had proven that with mathematics, coding, and a bit of imagination, the possibilities were limitless. They had learned from one another, grown together, and, most importantly, found joy in the world of numbers.\n",
        "\n",
        "And so, as the university chapters of their lives came to a close, Jerry, Eugene, Gangjoon, and Kaiwen knew one thing for certain: no matter where the future took them, they would always be a team. Together, they had created something remarkable, and in the process, they had discovered not just the power of data, but the power of friendship.\n",
        "'''\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "U_-kpD-NiEtM"
      },
      "outputs": [],
      "source": [
        "class VectorStore:\n",
        "    def __init__(self):\n",
        "        self.vectors = []\n",
        "        self.texts = []\n",
        "\n",
        "    def upsert(self, embedding, text):\n",
        "        self.vectors.extend(embedding)\n",
        "        self.texts.extend(text)\n",
        "\n",
        "    def pretty_print_contexts(self, top_k_results):\n",
        "        # Print the header\n",
        "        print(f\"{'Text':<50} {'Cosine Similarity Score':<25}\")\n",
        "        print(\"=\"*95)\n",
        "\n",
        "        # Iterate over each result and print it formatted\n",
        "        for result in top_k_results:\n",
        "            text = result[\"Text\"]\n",
        "            similarity_score = f\"{result['Cosine Similarity Score']:.4f}\"  # Format score to 4 decimal places\n",
        "\n",
        "            # Print each result with proper formatting\n",
        "            print(f\"{text:<50} {similarity_score:<25}\")\n",
        "\n",
        "    def retrieve(self, query_embedding, print_contexts, top_k=3):\n",
        "        '''This method is responsible for retrieving the top k vectors from the vector store'''\n",
        "        # np.dot() handles the matrix multiplication and computes the dot product between each vector in self.vectors and query_embedding\n",
        "        dot_products = np.dot(vector_store.vectors, query_embedding)\n",
        "\n",
        "        # Calculating the normalised vector of the query embedding\n",
        "        normalised_query_embedding = np.linalg.norm(query_embedding)\n",
        "\n",
        "        # Calculating the normalised vectors of each vector in vector store, is a 2D array\n",
        "        normalised_vector_embeddings = np.linalg.norm(self.vectors, axis=1)\n",
        "\n",
        "        # Calculate cosine similarity for each vector\n",
        "        cosine_similarities = dot_products / (normalised_query_embedding * normalised_vector_embeddings)\n",
        "\n",
        "        # using np.argsort to sort the similar vectors by their index\n",
        "        sorted_indices = np.argsort(cosine_similarities)\n",
        "\n",
        "        # Get indices of the top 3 most similar vectors\n",
        "        top_k_similar_indices = sorted_indices[-top_k:]\n",
        "\n",
        "        top_k_results = []\n",
        "        for i in range(len(top_k_similar_indices)):\n",
        "            similar_vector   = self.vectors[top_k_similar_indices[i]]\n",
        "            similar_text     = self.texts[i]\n",
        "            similarity_score = cosine_similarities[i]\n",
        "\n",
        "            top_k_results.append({\"Vector\": similar_vector,\n",
        "                                  \"Text\": similar_text,\n",
        "                                  \"Cosine Similarity Score\": similarity_score}\n",
        "                        )\n",
        "        if print_contexts:\n",
        "            self.pretty_print_contexts(top_k_results)\n",
        "\n",
        "        return sorted(top_k_results, reverse=True, key=lambda x: x['Cosine Similarity Score'])\n",
        "\n",
        "\n",
        "    def filter_similar_texts(self, retrieval_results):\n",
        "        similar_contexts = ''''''\n",
        "\n",
        "        for result in retrieval_results:\n",
        "            similar_contexts += result[\"Text\"]+\"\\n\\n\"\n",
        "\n",
        "        return similar_contexts\n",
        "\n",
        "\n",
        "class TextSplitter():\n",
        "\n",
        "    def __init__(self, delimiters=None):\n",
        "        if delimiters is None:\n",
        "            # Use \\n\\n as delimiters for splitting if a delimiter is not specified by the user\n",
        "            self.delimiters = \"\\n\\n\"\n",
        "        else:\n",
        "            self.delimiters = delimiters\n",
        "\n",
        "    def split_text(self, text):\n",
        "        '''This method takes in text and splits the text according to the delimiters specified'''\n",
        "        # Here we are splitting by paragraphs\n",
        "        chunks = text.split(self.delimiters)\n",
        "\n",
        "        stripped_chunks =  []\n",
        "        for chunk in chunks:\n",
        "            cleaned_chunk = chunk.strip()\n",
        "            # If this chunk is not simply an empty string\n",
        "            if cleaned_chunk:\n",
        "                stripped_chunks.append(chunk)\n",
        "\n",
        "        return stripped_chunks\n",
        "\n",
        "\n",
        "class EmbeddingModel():\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "        self.embeddings = []\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def generate_embeddings(self, texts: list):\n",
        "        # generate embeddings of all the texts, no for loop is necessary\n",
        "        embeddings = self.model.encode(texts)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "\n",
        "class chatbot():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.client = Groq(api_key = GROQ_API_KEY)\n",
        "\n",
        "    def build_prompt(self, user_query, context):\n",
        "        prompt = f\"Using the context provided, answer the question.\\n\\nContext:{context}\\n\\nQuestion:{user_query}\"\n",
        "        return prompt\n",
        "\n",
        "    def fetch_response(self, prompt, system_prompt):\n",
        "        # use chat completion function and insert this here\n",
        "        stream = self.client.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": system_prompt\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,  # Control the randomness of the output (lower means less random)\n",
        "            max_tokens=1024,  # Limit the response length\n",
        "            top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
        "            stream=True,  # Enable streaming of the response chunks\n",
        "            stop=None\n",
        "        )\n",
        "\n",
        "        # initialize an empty string to accumulate the response content\n",
        "        # answer = stream.choices[0].message.content\n",
        "        print(\"ChatGPT: \", end=\"\")\n",
        "        for chunk in stream:\n",
        "            print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "\n",
        "    def chat(self):\n",
        "        print(\"Welcome to the SDS Story chatbot! Feel free to ask me anything about the SDS workshop members!\")\n",
        "\n",
        "        while True:\n",
        "            query = input('User: ')\n",
        "\n",
        "            if query.lower() in ['exit', 'quit']:\n",
        "                print(\"Thank you for chatting!\\n\")\n",
        "                break\n",
        "\n",
        "            query_embedding = embedding_model.generate_embeddings([query])[0]\n",
        "            chatgpt.fetch_response(query, CHAT_SYSTEM_PROMPT)\n",
        "\n",
        "\n",
        "    def rag_pipeline(self, contextual_knowledge):\n",
        "        text_chunks = text_splitter.split_text(contextual_knowledge)\n",
        "        text_embeddings = embedding_model.generate_embeddings(text_chunks)\n",
        "        vector_store.upsert(text_embeddings, text_chunks)\n",
        "\n",
        "        print(\"Welcome to the SDS Story chatbot! Feel free to ask me anything about the SDS workshop members!\")\n",
        "        time.sleep(1)\n",
        "        while True:\n",
        "            query = input('User: ')\n",
        "\n",
        "            if query.lower() in ['exit', 'quit']:\n",
        "                print(\"Thank you for chatting!\\n\")\n",
        "                break\n",
        "\n",
        "            query_embedding = embedding_model.generate_embeddings([query])[0]\n",
        "\n",
        "            retrieved_results = vector_store.retrieve(query_embedding, print_contexts=False)\n",
        "            contexts_to_llm = vector_store.filter_similar_texts(retrieved_results)\n",
        "            prompt = chatgpt.build_prompt(query, contexts_to_llm)\n",
        "            chatgpt.fetch_response(prompt, RAG_SYSTEM_PROPMT)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store = VectorStore()\n",
        "text_splitter = TextSplitter()\n",
        "embedding_model = EmbeddingModel(model_name= EMBEDDING_MODEL)\n",
        "chatgpt = chatbot()"
      ],
      "metadata": {
        "id": "OhHrEr4Piw4B"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt.rag_pipeline(story)"
      ],
      "metadata": {
        "id": "pBVqVUWh4vVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chatgpt.chat()"
      ],
      "metadata": {
        "id": "86LKfa8877Lv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}