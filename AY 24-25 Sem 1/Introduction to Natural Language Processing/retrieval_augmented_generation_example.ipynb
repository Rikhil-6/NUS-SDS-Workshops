{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzu5b9bJiEtG"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG) from scratch: \n",
        "## A comparison between a RAG powered chatbot and normal Chatbot\n",
        "\n",
        "#### Start of Naive RAG process:\n",
        "\n",
        "- Chunking\n",
        "- Dense Embedding of text.\n",
        "- Makeshift storing of embeddings in a dictionary (use this as a vector store python class)\n",
        "- With same query, embed it.\n",
        "- Show retrieval from makeshift vector store using cosine similarity between query vector and vectors in vector store\n",
        "- Add context to the chatbot (tell it to refer to the context provided)\n",
        "- Show the comparison between raw LLM response and RAG pipeline.\n",
        "\n",
        "#### Additional/ Advanced steps to enhance your RAG pipeline:\n",
        "\n",
        "- Query re-writing\n",
        "- Re ranking using cross encoder\n",
        "- Dynamic Embedding model fine-tuning\n",
        "- Hybrid Search using BM25/ TF-IDF\n",
        "- LLM Guardrails/ Query intention\n",
        "\n",
        "#### Some practical applications:\n",
        "\n",
        "- RAG for your school notes (eg modules like HSI, where control F (in this case vector search) could help greatly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZPOCsiZvfWU",
        "outputId": "f90f7cfc-2fd1-4520-f452-e1e5fc3e948f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from groq) (0.27.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers\n",
        "!pip install groq\n",
        "\n",
        "# numpy needed for vector operations\n",
        "import numpy as np\n",
        "# regex libary needed for delimiter parsing furing text splitting\n",
        "import re\n",
        "# sentence_transformers library needed for loading our embedding model\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# groq library for our \"chatgpt\"\n",
        "from groq import Groq\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "dloB3T-XiMsK"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "GROQ_API_KEY= userdata.get('GROQ_API_KEY')\n",
        "CHAT_MODEL = \"llama3-70b-8192\"\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "RAG_SYSTEM_PROPMT = \"You are a helpful, very cheerful and very bubbbly assistant who only answers based on the contextual information provided, and nothing else. If you are unsure, say that you are unsure due to a lack of information.\"\n",
        "CHAT_SYSTEM_PROMPT = \"You are a helpful assistant who answers question factually. If you are unsure, say that you are unsure due to a lack of information.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "OGTXoPWDiEtK"
      },
      "outputs": [],
      "source": [
        "# read in contextual_knowledge \n",
        "with open('story.txt', 'r') as file:\n",
        "    contextual_knowledge = file.read()  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "U_-kpD-NiEtM"
      },
      "outputs": [],
      "source": [
        "class VectorStore:\n",
        "    '''Our Vector store that stores the contextual knowledge and their corresponding vector embeddings.'''\n",
        "    def __init__(self):\n",
        "        self.vectors = []\n",
        "        self.texts = []\n",
        "\n",
        "    def upsert(self, embedding, text):\n",
        "        self.vectors.extend(embedding)\n",
        "        self.texts.extend(text)\n",
        "\n",
        "    def pretty_print_contexts(self, top_k_results):\n",
        "        print(f\"{'Text':<50} {'Cosine Similarity Score':<25}\")\n",
        "        print(\"=\"*95)\n",
        "\n",
        "        # loop over each result and print it formatted\n",
        "        for result in top_k_results:\n",
        "            text = result[\"Text\"]\n",
        "            similarity_score = f\"{result['Cosine Similarity Score']:.4f}\"  # Format score to 4 decimal places\n",
        "\n",
        "            # Print each result with proper formatting\n",
        "            print(f\"{text:<50} {similarity_score:<25}\")\n",
        "\n",
        "    def retrieve(self, query_embedding, print_contexts, top_k=3):\n",
        "        '''This method is responsible for retrieving the top k vectors from the vector store'''\n",
        "        # np.dot() handles the matrix multiplication and computes the dot product between each vector in self.vectors and query_embedding\n",
        "        dot_products = np.dot(vector_store.vectors, query_embedding)\n",
        "\n",
        "        # Calculating the normalised vector of the query embedding\n",
        "        normalised_query_embedding = np.linalg.norm(query_embedding)\n",
        "\n",
        "        # Calculating the normalised vectors of each vector in vector store, is a 2D array\n",
        "        normalised_vector_embeddings = np.linalg.norm(self.vectors, axis=1)\n",
        "\n",
        "        # Calculate cosine similarity for each vector\n",
        "        cosine_similarities = dot_products / (normalised_query_embedding * normalised_vector_embeddings)\n",
        "\n",
        "        # using np.argsort to sort the similar vectors by their index\n",
        "        sorted_indices = np.argsort(cosine_similarities)\n",
        "\n",
        "        # Get indices of the top 3 most similar vectors\n",
        "        top_k_similar_indices = sorted_indices[-top_k:]\n",
        "\n",
        "        top_k_results = []\n",
        "        for i in range(len(top_k_similar_indices)):\n",
        "            similar_vector   = self.vectors[top_k_similar_indices[i]]\n",
        "            similar_text     = self.texts[i]\n",
        "            similarity_score = cosine_similarities[i]\n",
        "\n",
        "            top_k_results.append({\"Vector\": similar_vector,\n",
        "                                  \"Text\": similar_text,\n",
        "                                  \"Cosine Similarity Score\": similarity_score}\n",
        "                                )\n",
        "        if print_contexts:\n",
        "            self.pretty_print_contexts(top_k_results)\n",
        "\n",
        "        return sorted(top_k_results, reverse=True, key=lambda x: x['Cosine Similarity Score'])\n",
        "\n",
        "\n",
        "    def filter_similar_texts(self, retrieval_results):\n",
        "        similar_contexts = ''''''\n",
        "\n",
        "        # Loop through the top k results and append the texts\n",
        "        for result in retrieval_results:\n",
        "            similar_contexts += result[\"Text\"]+\"\\n\\n\"\n",
        "\n",
        "        return similar_contexts\n",
        "\n",
        "\n",
        "class TextSplitter():\n",
        "    '''Our object that enables us to split the texts according to how we want it to split.'''\n",
        "    def __init__(self, delimiters=None):\n",
        "        if delimiters is None:\n",
        "            # Use \\n\\n as delimiters for splitting if a delimiter is not specified by the user\n",
        "            self.delimiters = \"\\n\\n\"\n",
        "        else:\n",
        "            self.delimiters = delimiters\n",
        "\n",
        "    def split_text(self, text):\n",
        "        '''This method takes in text and splits the text according to the delimiters specified'''\n",
        "        # Here we are splitting by paragraphs\n",
        "        chunks = text.split(self.delimiters)\n",
        "\n",
        "        stripped_chunks =  []\n",
        "        for chunk in chunks:\n",
        "            cleaned_chunk = chunk.strip()\n",
        "            # If this chunk is not simply an empty string\n",
        "            if cleaned_chunk:\n",
        "                stripped_chunks.append(chunk)\n",
        "\n",
        "        return stripped_chunks\n",
        "\n",
        "\n",
        "class EmbeddingModel():\n",
        "    '''Our embedding model we will use for the encoding texts in the RAG pipeline'''\n",
        "    def __init__(self, model_name):\n",
        "        self.embeddings = []\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "    def generate_embeddings(self, texts: list):\n",
        "        # generate embeddings of all the texts, no for loop is necessary due to parallelization feature\n",
        "        embeddings = self.model.encode(texts)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "\n",
        "class chatbot():\n",
        "    '''This class is responsible for all chatbot related functions'''\n",
        "    def __init__(self):\n",
        "        self.client = Groq(api_key = GROQ_API_KEY)\n",
        "\n",
        "    def build_prompt(self, user_query, context):\n",
        "        prompt = f\"Using the context provided, answer the question.\\n\\nContext:{context}\\n\\nQuestion:{user_query}\"\n",
        "        return prompt\n",
        "\n",
        "    def fetch_response(self, prompt, system_prompt):\n",
        "        stream = self.client.chat.completions.create(\n",
        "            model=CHAT_MODEL,\n",
        "            messages=[\n",
        "                {\n",
        "                    # initialise system prompt\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": system_prompt\n",
        "                },\n",
        "                {\n",
        "                    # initialise users prompt\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": prompt\n",
        "                }\n",
        "            ],\n",
        "            temperature=0,  # Control the randomness of the output (lower means less random)\n",
        "            max_tokens=1024,  # Limit the response length\n",
        "            top_p=1,  # Nucleus sampling parameter (1 means only the most likely tokens are considered)\n",
        "            stream=True,  # Enable streaming of the response chunks\n",
        "            stop=None\n",
        "        )\n",
        "\n",
        "        print(\"ChatGPT: \", end=\"\")\n",
        "        for chunk in stream:\n",
        "            print(chunk.choices[0].delta.content, end=\"\")\n",
        "\n",
        "\n",
        "    def chat(self):\n",
        "        print(\"Welcome to ChatGPT! Feel free to ask me anything!\")\n",
        "        time.sleep(1)\n",
        "\n",
        "        while True:\n",
        "            query = input('User: ')\n",
        "\n",
        "            if query.lower() in ['exit', 'quit']:\n",
        "                print(\"Thank you for chatting!\\n\")\n",
        "                break\n",
        "            chatgpt.fetch_response(query, CHAT_SYSTEM_PROMPT)\n",
        "\n",
        "\n",
        "    def rag_pipeline(self, contextual_knowledge):\n",
        "        # split the text accorinding to delimiters\n",
        "        text_chunks = text_splitter.split_text(contextual_knowledge)\n",
        "        # generate embeddings of desired text\n",
        "        text_embeddings = embedding_model.generate_embeddings(text_chunks)\n",
        "        # upsert the vector embeddings and text chunks to the vector store\n",
        "        vector_store.upsert(text_embeddings, text_chunks)\n",
        "\n",
        "        print(\"Welcome to the SDS Story chatbot! Feel free to ask me anything about the SDS workshop members!\")\n",
        "        time.sleep(1)\n",
        "        while True:\n",
        "            # get users input\n",
        "            query = input('User: ')\n",
        "\n",
        "            if query.lower() in ['exit', 'quit']:\n",
        "                print(\"Thank you for chatting!\\n\")\n",
        "                break\n",
        "            \n",
        "            # generate the vector emebeddings of the users query\n",
        "            query_embedding = embedding_model.generate_embeddings([query])[0]\n",
        "            # conduct the similarity search within the vector store\n",
        "            retrieved_results = vector_store.retrieve(query_embedding, print_contexts=False)\n",
        "            # filter out the similar texts from the retrieved results\n",
        "            contexts_to_llm = vector_store.filter_similar_texts(retrieved_results)\n",
        "            # build the prompt to include the context retrieved\n",
        "            prompt = chatgpt.build_prompt(query, contexts_to_llm)\n",
        "            chatgpt.fetch_response(prompt, RAG_SYSTEM_PROPMT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "OhHrEr4Piw4B"
      },
      "outputs": [],
      "source": [
        "vector_store    = VectorStore()\n",
        "text_splitter   = TextSplitter()\n",
        "embedding_model = EmbeddingModel(model_name= EMBEDDING_MODEL)\n",
        "chatgpt         = chatbot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Whether the rag pipeline runs or the normal chat function runs depends on the users needs and query!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBVqVUWh4vVp"
      },
      "outputs": [],
      "source": [
        "chatgpt.rag_pipeline(contextual_knowledge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86LKfa8877Lv"
      },
      "outputs": [],
      "source": [
        "chatgpt.chat()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
